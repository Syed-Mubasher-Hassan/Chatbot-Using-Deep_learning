{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM,GRU, Embedding,Dropout, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "KzUHWxmVMt27"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the JSON dataset\n",
        "\n",
        "with open('/content/drive/MyDrive/train.json') as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "U0sIovpdBhT7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "training_sentences = []  # Agent 1' and 2 messages\n",
        "training_labels = []     # Sentiments\n",
        "responses = []           # Agent 2's responses\n",
        "labels = []              # Unique sentiment labels\n",
        "\n",
        "for message_id, message_data in data.items():\n",
        "    for content in message_data['content']:\n",
        "        agent_message = content['message']\n",
        "        sentiment = content['sentiment']\n",
        "        agent = content['agent']\n",
        "        training_sentences.append(agent_message)\n",
        "        training_labels.append(sentiment)\n",
        "\n",
        "        if agent == 'agent_2':\n",
        "            responses.append(agent_message)\n",
        "\n",
        "        # Check for unique labels\n",
        "        if sentiment not in labels:\n",
        "            labels.append(sentiment)\n",
        "num_classes = len(labels)\n",
        "\n",
        "# Now you have training sentences (Agent 1's messages), labels (sentiments), responses (Agent 2's messages), and unique labels in the 'labels' list\n"
      ],
      "metadata": {
        "id": "hu8EafDubdy3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "training_sentences = []  # Agent 1' and 2 messages\n",
        "training_labels = []     # Sentiments\n",
        "responses = []           # Agent 2's responses\n",
        "labels = []              # Unique sentiment labels\n",
        "\n",
        "for message_id, message_data in data.items():\n",
        "    for content in message_data['content']:\n",
        "        agent_message = content['message']\n",
        "        sentiment = content['sentiment']\n",
        "        agent = content['agent']\n",
        "        training_sentences.append(agent_message)\n",
        "        training_labels.append(sentiment)\n",
        "\n",
        "        if agent == 'agent_2':\n",
        "            responses.append(agent_message)\n",
        "\n",
        "        # Check for unique labels\n",
        "        if sentiment not in labels:\n",
        "            labels.append(sentiment)\n",
        "num_classes = len(labels)\n",
        "\n",
        "# Now you have training sentences (Agent 1's messages), labels (sentiments), responses (Agent 2's messages), and unique labels in the 'labels' list\n"
      ],
      "metadata": {
        "id": "RPhtFHyAM2kV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(labels))\n",
        "print(len(training_sentences))\n",
        "print(len(training_labels))\n",
        "print(len(responses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOGWJKbaNAdr",
        "outputId": "53e23a20-0ad9-45a8-e3b5-9f7313ed206b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "188378\n",
            "188378\n",
            "91174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_sentences=training_sentences[:60000]\n",
        "training_labels=training_labels[:60000]\n",
        "responses=responses[:60000]"
      ],
      "metadata": {
        "id": "jF-1rOjMND4A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lbl_encoder = LabelEncoder()\n",
        "lbl_encoder.fit(training_labels)\n",
        "training_labels = lbl_encoder.transform(training_labels)"
      ],
      "metadata": {
        "id": "H_CvHu1vNNtI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 40000\n",
        "embedding_dim = 200\n",
        "max_len = 512\n",
        "oov_token = \"<OOV>\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)"
      ],
      "metadata": {
        "id": "VhiijCXeNRoL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and temporary data (combining validation and testing)\n",
        "training_sentences, temp_sentences, training_labels, temp_labels = train_test_split(\n",
        "    padded_sequences, training_labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Split the temporary data into validation and testing\n",
        "validation_sentences, test_sentences, validation_labels, test_labels = train_test_split(\n",
        "    temp_sentences, temp_labels, test_size=0.1, random_state=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JtJiVGNcNcZX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(LSTM(128, return_sequences=True))  # You can adjust the number of LSTM units as needed\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pjp7dISmNkoN",
        "outputId": "58fce443-6645-403f-a432-2f1d6ea3040c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 512, 200)          8000000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 512, 128)          168448    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512, 128)          0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 512, 256)          394240    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512, 256)          0         \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 512)               1574912   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 2056      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10270984 (39.18 MB)\n",
            "Trainable params: 10270984 (39.18 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    training_sentences, training_labels,\n",
        "    epochs=10,  # Adjust the number of epochs as needed\n",
        "    validation_data=(validation_sentences, validation_labels),\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWYAR3-1OCqO",
        "outputId": "ff0bccd7-b43e-440d-9c60-8f29243dfbc5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1313/1313 [==============================] - 222s 160ms/step - loss: 1.3743 - accuracy: 0.4326 - val_loss: 1.3177 - val_accuracy: 0.4614\n",
            "Epoch 2/10\n",
            "1313/1313 [==============================] - 203s 155ms/step - loss: 1.2518 - accuracy: 0.4972 - val_loss: 1.3044 - val_accuracy: 0.4690\n",
            "Epoch 3/10\n",
            "1313/1313 [==============================] - 203s 155ms/step - loss: 1.1047 - accuracy: 0.5742 - val_loss: 1.3550 - val_accuracy: 0.4706\n",
            "Epoch 4/10\n",
            "1313/1313 [==============================] - 203s 154ms/step - loss: 0.9331 - accuracy: 0.6529 - val_loss: 1.5179 - val_accuracy: 0.4452\n",
            "Epoch 5/10\n",
            "1313/1313 [==============================] - 185s 141ms/step - loss: 0.7732 - accuracy: 0.7180 - val_loss: 1.6065 - val_accuracy: 0.4477\n",
            "Epoch 6/10\n",
            "1313/1313 [==============================] - 202s 154ms/step - loss: 0.6466 - accuracy: 0.7650 - val_loss: 1.8408 - val_accuracy: 0.4304\n",
            "Epoch 7/10\n",
            "1313/1313 [==============================] - 201s 153ms/step - loss: 0.5482 - accuracy: 0.8010 - val_loss: 2.0252 - val_accuracy: 0.4448\n",
            "Epoch 8/10\n",
            "1313/1313 [==============================] - 202s 153ms/step - loss: 0.4755 - accuracy: 0.8264 - val_loss: 2.1927 - val_accuracy: 0.4234\n",
            "Epoch 9/10\n",
            "1313/1313 [==============================] - 201s 153ms/step - loss: 0.4166 - accuracy: 0.8487 - val_loss: 2.4731 - val_accuracy: 0.4212\n",
            "Epoch 10/10\n",
            "1313/1313 [==============================] - 202s 154ms/step - loss: 0.3703 - accuracy: 0.8657 - val_loss: 2.5962 - val_accuracy: 0.4158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data when you're ready\n",
        "test_loss, test_accuracy = model.evaluate(test_sentences, test_labels)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xji-DeZgeoVX",
        "outputId": "03d85420-5be6-48c4-cc17-2ba04c2d13c7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 3s 49ms/step - loss: 2.5231 - accuracy: 0.4250\n",
            "Test Accuracy: 42.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Make predictions on the training data\n",
        "train_predictions = model.predict(training_sentences)\n",
        "\n",
        "# Convert the predictions from one-hot encoding to class labels\n",
        "train_predicted_labels = np.argmax(train_predictions, axis=1)\n",
        "\n",
        "# Generate a classification report for training data\n",
        "train_class_report = classification_report(training_labels, train_predicted_labels, target_names=labels)\n",
        "\n",
        "# Print the training classification report\n",
        "print(\"Training Classification Report:\\n\", train_class_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Jd0Am2knZIf",
        "outputId": "808c1d74-3521-4fe8-d6f4-a449404e2d89"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1313/1313 [==============================] - 56s 43ms/step\n",
            "Training Classification Report:\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Curious to dive deeper       0.79      0.68      0.73       168\n",
            "                 Happy       0.89      0.95      0.92     17780\n",
            "               Neutral       0.77      0.80      0.79       276\n",
            "             Surprised       0.74      0.39      0.51       231\n",
            "             Disgusted       0.88      0.82      0.85      6630\n",
            "                   Sad       0.89      0.89      0.89      9394\n",
            "               Fearful       0.73      0.93      0.82       614\n",
            "                 Angry       0.95      0.84      0.90      6907\n",
            "\n",
            "              accuracy                           0.89     42000\n",
            "             macro avg       0.83      0.79      0.80     42000\n",
            "          weighted avg       0.90      0.89      0.89     42000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(test_sentences)\n",
        "\n",
        "# Convert the predictions from one-hot encoding to class labels\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Generate a classification report\n",
        "class_report = classification_report(test_labels, predicted_labels, target_names=labels)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\\n\", class_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzXrViJimp31",
        "outputId": "0029abcf-e9ae-4090-a650-8e581896ddf4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 5s 44ms/step\n",
            "Classification Report:\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "Curious to dive deeper       0.00      0.00      0.00         9\n",
            "                 Happy       0.57      0.59      0.58       752\n",
            "               Neutral       0.00      0.00      0.00        11\n",
            "             Surprised       0.00      0.00      0.00        16\n",
            "             Disgusted       0.28      0.28      0.28       275\n",
            "                   Sad       0.36      0.39      0.37       419\n",
            "               Fearful       0.09      0.12      0.11        24\n",
            "                 Angry       0.33      0.26      0.29       294\n",
            "\n",
            "              accuracy                           0.42      1800\n",
            "             macro avg       0.20      0.21      0.20      1800\n",
            "          weighted avg       0.42      0.42      0.42      1800\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# to save the trained model\n",
        "model.save(\"chat_model\")\n",
        "\n",
        "import pickle\n",
        "\n",
        "# to save the fitted tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# to save the fitted label encoder\n",
        "with open('label_encoder.pickle', 'wb') as ecn_file:\n",
        "    pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "lJ6jZTonOM_h"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}