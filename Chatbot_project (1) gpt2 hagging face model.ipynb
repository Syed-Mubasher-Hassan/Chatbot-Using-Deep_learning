{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the JSON dataset\n",
        "\n",
        "with open('/content/drive/MyDrive/train.json') as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "U0sIovpdBhT7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty list to store the messages\n",
        "messages_list = []\n",
        "sentiments_list = []\n",
        "\n",
        "# Iterate through the data structure to extract messages\n",
        "for conversation_id, conversation_data in data.items():\n",
        "    for message_data in conversation_data['content']:\n",
        "        message = message_data['message']\n",
        "        sentiment = message_data['sentiment']\n",
        "        messages_list.append(message)\n",
        "        sentiments_list.append(sentiment)"
      ],
      "metadata": {
        "id": "hu8EafDubdy3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(messages_list))\n",
        "print(len(sentiments_list))"
      ],
      "metadata": {
        "id": "rcnswkvWBu62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "873cac15-22a0-42fb-c571-ee7fff92bb51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188378\n",
            "188378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(sentiments_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMlLfKSVYfWg",
        "outputId": "05cd238c-659b-4722-f46d-ac38c9a0908b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Angry', 'Surprised', 'Curious to dive deeper', 'Fearful', 'Sad', 'Happy', 'Disgusted', 'Neutral'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages_list[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxMZDYqjzv8c",
        "outputId": "8645da63-7542-41ff-cff6-fe4f98f1654e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Are you a fan of Google or Microsoft?',\n",
              " 'Both are excellent technology they are helpful in many ways. For the security purpose both are super.',\n",
              " \"I'm not  a huge fan of Google, but I use it a lot because I have to. I think they are a monopoly in some sense. \",\n",
              " 'Google provides online related services and products, which includes online ads, search engine and cloud computing.',\n",
              " \"Yeah, their services are good. I'm just not a fan of intrusive they can be on our personal lives. \",\n",
              " 'Google is leading the alphabet subsidiary and will continue to be the Umbrella company for Alphabet internet interest.',\n",
              " 'Did you know Google had hundreds of live goats to cut the grass in the past? \\n',\n",
              " 'It is very interesting. Google provide \"Chrome OS\" which is a light weight OS. Google provided a lot of hardware mainly in 2010 to 2015. ',\n",
              " 'I like Google Chrome. Do you use it as well for your browser? ',\n",
              " 'Yes.Google is the biggest search engine and Google service figure out top 100 website, including Youtube and Blogger.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages_list=messages_list[:5000]"
      ],
      "metadata": {
        "id": "75pm6KDbgbjX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "# define class for custom dataset\n",
        "class ChatData(Dataset):\n",
        "  def __init__(self,messages_list,tokenizer):\n",
        "    self.messages_list=messages_list\n",
        "\n",
        "\n",
        "    for idx,i in enumerate(self.messages_list):\n",
        "       try:\n",
        "          self.messages_list[idx]=\"<startofstring> \"+i+\" <bot>: \"+self.messages_list[idx+1]+\" <endofstring>\"\n",
        "       except:\n",
        "        break\n",
        "\n",
        "\n",
        "    print(self.messages_list[0])\n",
        "\n",
        "\n",
        "    self.text_encoded= tokenizer(self.messages_list,max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    self.input_ids=self.text_encoded['input_ids']\n",
        "    self.attention_mask=self.text_encoded['attention_mask']\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.messages_list)\n",
        "  def __getitem__(self, idx):\n",
        "    return (self.input_ids[idx],self.attention_mask[idx])\n",
        "\n"
      ],
      "metadata": {
        "id": "oXXNBpfUilu0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "Fd1gin9OmMjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import tqdm\n",
        "import torch\n",
        "\n",
        "# Define a function to train the model\n",
        "def train(ChatData, model, optim):\n",
        "    epochs = 10\n",
        "    for i in tqdm.tqdm(range(epochs)):\n",
        "        for x, a in ChatData:\n",
        "            x = x.to(device)\n",
        "            a = a.to(device)\n",
        "            optim.zero_grad()\n",
        "            loss = model(x, attention_mask=a, labels=x).loss\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "        torch.save(model.state_dict(), 'model_state.pt')\n",
        "        print(infer(\" I'm not  a huge fan of Google\"))\n",
        "\n",
        "# Define a function for inference\n",
        "def infer(inp):\n",
        "    inp = \"<startofstring> \" + inp + \" <bot>: \"\n",
        "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
        "    X = inp[\"input_ids\"].to(device)\n",
        "    a = inp[\"attention_mask\"].to(device)\n",
        "    output = model.generate(X, attention_mask=a)\n",
        "    output = tokenizer.decode(output[0])\n",
        "    return output\n",
        "\n",
        "# Choose the appropriate device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"gpu\"\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n",
        "                              \"bos_token\": \"<startofstring>\",\n",
        "                              \"eos_token\": \"<endofstring>\"})\n",
        "tokenizer.add_tokens([\"<bot>:\"])\n",
        "\n",
        "# Initialize the model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Resize the token embeddings\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(device)\n",
        "\n",
        "# Assuming you have a ChatData class for your data loading\n",
        "\n",
        "# Create DataLoader for ChatData\n",
        "ChatData = ChatData(messages_list, tokenizer)\n",
        "ChatData = DataLoader(ChatData, batch_size=32)\n",
        "\n",
        "# Set the model in training mode\n",
        "model.train()\n",
        "\n",
        "# Initialize the optimizer\n",
        "optim = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"Training....\")\n",
        "train(ChatData, model, optim)\n",
        "\n",
        "print(\"Infer from model:\")\n",
        "# while True:\n",
        "#     inp = input()\n",
        "#     print(infer(inp))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycxer-Mkls_t",
        "outputId": "9227e1d5-6d9d-4074-e2cc-6e5fe88b9e11"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50261. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startofstring> <startofstring> Are you a fan of Google or Microsoft? <bot>: Both are excellent technology they are helpful in many ways. For the security purpose both are super. <endofstring> <bot>: <startofstring> Both are excellent technology they are helpful in many ways. For the security purpose both are super. <bot>: I'm not  a huge fan of Google, but I use it a lot because I have to. I think they are a monopoly in some sense.  <endofstring> <endofstring>\n",
            "Training....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 10%|█         | 1/10 [02:47<25:11, 167.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startofstring> I'm not  a huge fan of Google <bot>: I'm not sure. I'm not sure\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 20%|██        | 2/10 [05:34<22:19, 167.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startofstring> I'm not  a huge fan of Google <bot>: I'm not  a big fan of Google\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 30%|███       | 3/10 [08:21<19:28, 166.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startofstring> I'm not  a huge fan of Google <bot>: I'm not  a big fan of Google\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 40%|████      | 4/10 [11:07<16:39, 166.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startofstring> I'm not  a huge fan of Google <bot>: I'm not  a big fan of Google\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 50%|█████     | 5/10 [13:54<13:53, 166.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startofstring> I'm not  a huge fan of Google <bot>: I'm not  a huge fan of Google\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 60%|██████    | 6/10 [16:41<11:07, 166.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startofstring> I'm not  a huge fan of Google <bot>: I'm not a huge fan of Google.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 70%|███████   | 7/10 [19:30<08:23, 167.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startofstring> I'm not  a huge fan of Google <bot>: I'm  a little more into the internet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 80%|████████  | 8/10 [22:18<05:35, 167.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startofstring> I'm not  a huge fan of Google <bot>: I just buy an Apple tshirt for not\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 90%|█████████ | 9/10 [25:05<02:47, 167.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startofstring> I'm not  a huge fan of Google <bot>: That's pretty cool. I can see why\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "100%|██████████| 10/10 [27:52<00:00, 167.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startofstring> I'm not  a huge fan of Google <bot>: I don't know why they are good for\n",
            "Infer from model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    inp = input()\n",
        "    if inp=='quit':\n",
        "      break\n",
        "    else:\n",
        "         print(infer(inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jQhiIX_m5JA",
        "outputId": "0955ffda-79dc-400c-ac63-88065d36b022"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<startofstring> hello <bot>: Hi, how are you? <endofstring> <bot>: <startofstring> Hi, how are you? <bot>: hello\n",
            "hello\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<startofstring> hello <bot>: Hi, how are you? <endofstring> <bot>: <startofstring> Hi, how are you? <bot>: Good\n",
            "do you know about google\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<startofstring> do you know about google <bot>: I love Google. Google even prefers dogs to cats. <endofstring> <bot>:\n",
            "I'm not  a huge fan of Google, but I use it a lot because I have to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 22, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<startofstring> I'm not  a huge fan of Google, but I use it a lot because I have to <bot>: Google\n",
            "but I use it a lot because I have to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<startofstring> but I use it a lot because I have to <bot>: I think it helps people to feel more\n",
            "what about microsoft\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<startofstring> what about microsoft <bot>: Yes, i did not know that. i wonder if it was in\n",
            "quit\n"
          ]
        }
      ]
    }
  ]
}